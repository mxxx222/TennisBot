#!/usr/bin/env python3
"""
ChromeDriver and Web Scraping Test
==================================

Test script to verify ChromeDriver is working correctly for web scraping.
"""

import asyncio
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import sys

def get_chrome_driver():
    """Get ChromeDriver with proper options"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--no-first-run')
    options.add_argument('--disable-default-apps')
    
    # Set user agent
    options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    try:
        driver = webdriver.Chrome(options=options)
        return driver
    except Exception as e:
        print(f"âŒ ChromeDriver setup failed: {e}")
        print("\nğŸ”§ Setup Instructions:")
        print("   macOS: brew install chromedriver")
        print("   Ubuntu: sudo apt install chromium-chromedriver")
        print("   Windows: Download from https://chromedriver.chromium.org/")
        return None

async def test_scraping():
    """Test web scraping functionality"""
    print("ğŸ§ª Testing web scraping setup...\n")
    
    # Setup ChromeDriver
    driver = get_chrome_driver()
    if not driver:
        return False
    
    try:
        # Test 1: Basic navigation
        print("1ï¸âƒ£ Test basic navigation...")
        driver.get('https://www.google.com')
        time.sleep(2)
        assert 'Google' in driver.title
        print("   âœ… Basic navigation works\n")
        
        # Test 2: JavaScript rendering
        print("2ï¸âƒ£ Test JavaScript rendering...")
        driver.get('https://httpbin.org/html')
        time.sleep(3)
        page_source = driver.page_source
        assert len(page_source) > 1000
        print("   âœ… JavaScript rendering works\n")
        
        # Test 3: Dynamic content
        print("3ï¸âƒ£ Test dynamic content loading...")
        driver.get('https://jsonplaceholder.typicode.com/')
        time.sleep(2)
        
        # Try to find some dynamic content
        title = driver.title
        assert len(title) > 0
        print(f"   âœ… Dynamic content loaded (title: {title})\n")
        
        # Test 4: BeautifulSoup parsing
        print("4ï¸âƒ£ Test HTML parsing...")
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        assert soup.find('html') is not None
        print("   âœ… HTML parsing works\n")
        
        # Test 5: Selenium waits
        print("5ï¸âƒ£ Test Selenium waits...")
        driver.get('https://httpbin.org/delay/1')
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, 'html')))
        print("   âœ… Selenium waits work\n")
        
        print("ğŸ‰ All scraping tests passed!")
        print("ğŸŒ Web scraping is ready for production!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False
        
    finally:
        driver.quit()

def test_scraper_classes():
    """Test if our scraper classes can be imported"""
    print("\nğŸ“¦ Testing scraper class imports...")
    
    try:
        # Test basic imports
        from src.scrapers.sports_scraper import SportsScraper
        print("   âœ… SportsScraper import works")
        
        from src.scrapers.scraping_utils import ScrapingUtils
        print("   âœ… ScrapingUtils import works")
        
        # Test scraper initialization
        scraper = SportsScraper()
        print("   âœ… SportsScraper instantiation works")
        
        print("âœ… All scraper classes ready!")
        return True
        
    except Exception as e:
        print(f"âŒ Import error: {e}")
        return False

def show_scraper_info():
    """Show information about available scrapers"""
    print("\nğŸ“Š Available Scrapers:")
    print("   ğŸ”¹ SofaScore Scraper: xG data, momentum analysis")
    print("   ğŸ”¹ FotMob Scraper: Lineups, injuries, team news")
    print("   ğŸ”¹ FlashScore Scraper: Live events, ultra-fast updates")
    print("   ğŸ”¹ Betfury Scraper: Live odds, movement tracking")
    print("   ğŸ”¹ Understat Scraper: Advanced xG models")
    print("   ğŸ”¹ API Football Scraper: Base statistics")
    
    print("\nğŸš€ Scraping Capabilities:")
    print("   âœ… Concurrent data collection from multiple sources")
    print("   âœ… Headless browser automation")
    print("   âœ… JavaScript content handling")
    print("   âœ… Anti-detection measures")
    print("   âœ… Rate limiting and error handling")

if __name__ == "__main__":
    print("ğŸŒ ChromeDriver & Web Scraping Test Suite")
    print("=" * 50)
    
    # Test scraper imports first
    import_success = test_scraper_classes()
    
    # Test web scraping
    scraping_success = asyncio.run(test_scraping())
    
    # Show information
    show_scraper_info()
    
    # Final result
    print("\n" + "=" * 50)
    if import_success and scraping_success:
        print("ğŸ‰ ALL TESTS PASSED!")
        print("âœ… ChromeDriver: Ready")
        print("âœ… Web Scraping: Ready")
        print("âœ… Scraper Classes: Ready")
        print("\nğŸš€ System ready for multi-source scraping!")
        sys.exit(0)
    else:
        print("âŒ Some tests failed")
        print("ğŸ”§ Please fix issues before proceeding")
        sys.exit(1)