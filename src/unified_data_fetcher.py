#!/usr/bin/env python3
"""
üìä UNIFIED DATA FETCHER
=======================
Yhdistetty datan hakurakenne joka hakee automaattisesti kaiken tarvittavan datan
ja tilastot useista API-l√§hteist√§ kaikille lajeille.

Features:
- üîÑ Automaattinen datan hakeminen useista l√§hteist√§
- üìä Tilastojen ker√§ys (120+ eri tilastoa)
- üí∞ Kertoimien aggregaatio
- üéØ Historiallisen datan hakeminen
- üåê Ulkoisten tekij√∂iden ker√§ys
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import yaml
from pathlib import Path

# Import existing integrations
try:
    from src.odds_api_integration import OddsAPIIntegration, OddsData
    ODDS_API_AVAILABLE = True
except ImportError:
    ODDS_API_AVAILABLE = False
    OddsAPIIntegration = None

try:
    from src.api_football_scraper import APIFootballScraper
    API_FOOTBALL_AVAILABLE = True
except ImportError:
    API_FOOTBALL_AVAILABLE = False
    APIFootballScraper = None

try:
    from src.multi_sport_prematch_scraper import MultiSportPrematchScraper
    MULTI_SPORT_AVAILABLE = True
except ImportError:
    MULTI_SPORT_AVAILABLE = False
    MultiSportPrematchScraper = None

logger = logging.getLogger(__name__)


@dataclass
class UnifiedMatchData:
    """Yhdistetty otteludata kaikista l√§hteist√§"""
    match_id: str
    sport: str
    league: str
    home_team: str
    away_team: str
    match_time: datetime
    venue: Optional[str] = None
    
    # Data from different sources
    odds_data: Optional[Dict[str, Any]] = None
    team_stats: Optional[Dict[str, Any]] = None
    player_stats: Optional[Dict[str, Any]] = None
    historical_data: Optional[Dict[str, Any]] = None
    external_factors: Optional[Dict[str, Any]] = None
    
    # Metadata
    sources: List[str] = None
    data_quality: float = 0.0
    fetched_at: datetime = None
    
    def __post_init__(self):
        if self.sources is None:
            self.sources = []
        if self.fetched_at is None:
            self.fetched_at = datetime.now()


class UnifiedDataFetcher:
    """
    Yhdistetty datan hakurakenne joka hakee dataa useista l√§hteist√§
    """
    
    def __init__(self, config_path: str = None):
        """
        Initialize Unified Data Fetcher
        
        Args:
            config_path: Path to configuration file
        """
        logger.info("üìä Initializing Unified Data Fetcher...")
        
        # Load configuration
        if config_path is None:
            config_path = Path(__file__).parent.parent / "config" / "unified_data_config.yaml"
        
        self.config = self._load_config(config_path)
        
        # Initialize data source integrations
        self.odds_api = None
        self.api_football = None
        self.multi_sport_scraper = None
        
        self._initialize_data_sources()
        
        logger.info("‚úÖ Unified Data Fetcher initialized")
    
    def _load_config(self, config_path: Path) -> Dict[str, Any]:
        """Load configuration from YAML file"""
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Replace environment variables
            if 'api_keys' in config:
                for key, value in config['api_keys'].items():
                    if isinstance(value, str) and value.startswith('${'):
                        env_var = value[2:-1]
                        config['api_keys'][key] = os.getenv(env_var, '')
            
            return config
        except Exception as e:
            logger.error(f"‚ùå Error loading config: {e}")
            return {}
    
    def _initialize_data_sources(self):
        """Initialize all data source integrations"""
        
        # Odds API
        if ODDS_API_AVAILABLE and self.config.get('data_sources', {}).get('odds_api', {}).get('enabled', False):
            try:
                api_key = self.config.get('api_keys', {}).get('odds_api')
                if api_key:
                    self.odds_api = OddsAPIIntegration(api_key=api_key)
                    logger.info("‚úÖ Odds API initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Odds API initialization failed: {e}")
        
        # API Football
        if API_FOOTBALL_AVAILABLE and self.config.get('data_sources', {}).get('api_football', {}).get('enabled', False):
            try:
                api_key = self.config.get('api_keys', {}).get('api_football')
                if api_key:
                    api_config = {
                        'api_key': api_key,
                        'rate_limit': self.config.get('data_sources', {}).get('api_football', {}).get('rate_limit', 100)
                    }
                    self.api_football = APIFootballScraper(api_config)
                    logger.info("‚úÖ API Football initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è API Football initialization failed: {e}")
        
        # Multi-Sport Scraper
        if MULTI_SPORT_AVAILABLE:
            try:
                self.multi_sport_scraper = MultiSportPrematchScraper()
                logger.info("‚úÖ Multi-Sport Scraper initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Multi-Sport Scraper initialization failed: {e}")
    
    async def fetch_all_matches(self, sports: List[str] = None, date_range: Tuple[datetime, datetime] = None) -> List[UnifiedMatchData]:
        """
        Hae kaikki ottelut useista l√§hteist√§
        
        Args:
            sports: Lista lajeista (jos None, k√§yt√§ kaikkia konfiguroituja)
            date_range: (start_date, end_date) tuple
            
        Returns:
            Lista UnifiedMatchData objekteja
        """
        logger.info("üîç Fetching all matches from multiple sources...")
        
        if sports is None:
            sports = [sport for sport, config in self.config.get('sports', {}).items() 
                     if config.get('enabled', False)]
        
        if date_range is None:
            start_date = datetime.now()
            end_date = start_date + timedelta(days=7)
            date_range = (start_date, end_date)
        
        all_matches = []
        
        # Fetch from each source in parallel
        tasks = []
        
        # Odds API
        if self.odds_api:
            tasks.append(self._fetch_from_odds_api(sports, date_range))
        
        # API Football (for football only)
        if self.api_football and 'football' in sports:
            tasks.append(self._fetch_from_api_football(date_range))
        
        # Multi-Sport Scraper
        if self.multi_sport_scraper:
            tasks.append(self._fetch_from_multi_sport_scraper(sports, date_range))
        
        # Execute all fetches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Combine results
        for result in results:
            if isinstance(result, list):
                all_matches.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå Error fetching data: {result}")
        
        # Merge duplicate matches
        merged_matches = self._merge_duplicate_matches(all_matches)
        
        logger.info(f"‚úÖ Fetched {len(merged_matches)} unique matches")
        return merged_matches
    
    async def _fetch_from_odds_api(self, sports: List[str], date_range: Tuple[datetime, datetime]) -> List[UnifiedMatchData]:
        """Fetch matches from Odds API"""
        if not self.odds_api:
            return []
        
        logger.info("üìä Fetching from Odds API...")
        
        # Map sports to Odds API sport keys
        sport_mapping = {
            'tennis': ['tennis_atp', 'tennis_wta'],
            'football': ['soccer_epl', 'soccer_spain_la_liga', 'soccer_germany_bundesliga', 
                        'soccer_italy_serie_a', 'soccer_france_ligue_one', 'soccer_uefa_champs_league'],
            'basketball': ['basketball_nba'],
            'ice_hockey': ['icehockey_nhl']
        }
        
        odds_sports = []
        for sport in sports:
            if sport in sport_mapping:
                odds_sports.extend(sport_mapping[sport])
        
        if not odds_sports:
            return []
        
        try:
            odds_data_list = await self.odds_api.get_live_odds(odds_sports, ['h2h', 'spreads', 'totals'])
            
            matches = []
            for odds_data in odds_data_list:
                # Convert OddsData to UnifiedMatchData
                match = UnifiedMatchData(
                    match_id=f"odds_api_{odds_data.sport_key}_{odds_data.home_team}_{odds_data.away_team}",
                    sport=self._normalize_sport(odds_data.sport_key),
                    league=odds_data.sport_title,
                    home_team=odds_data.home_team,
                    away_team=odds_data.away_team,
                    match_time=odds_data.commence_time,
                    odds_data={
                        'best_odds': odds_data.best_odds,
                        'arbitrage_opportunity': odds_data.arbitrage_opportunity,
                        'value_bets': odds_data.value_bets,
                        'market_margin': odds_data.market_margin,
                        'bookmakers': odds_data.bookmakers
                    },
                    sources=['odds_api'],
                    data_quality=0.8
                )
                matches.append(match)
            
            logger.info(f"‚úÖ Odds API: {len(matches)} matches")
            return matches
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching from Odds API: {e}")
            return []
    
    async def _fetch_from_api_football(self, date_range: Tuple[datetime, datetime]) -> List[UnifiedMatchData]:
        """Fetch matches from API Football"""
        if not self.api_football:
            return []
        
        logger.info("‚öΩ Fetching from API Football...")
        
        try:
            # This would use API Football scraper
            # For now, return empty list as implementation depends on API Football scraper
            matches = []
            logger.info(f"‚úÖ API Football: {len(matches)} matches")
            return matches
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching from API Football: {e}")
            return []
    
    async def _fetch_from_multi_sport_scraper(self, sports: List[str], date_range: Tuple[datetime, datetime]) -> List[UnifiedMatchData]:
        """Fetch matches from Multi-Sport Scraper"""
        if not self.multi_sport_scraper:
            return []
        
        logger.info("üï∑Ô∏è Fetching from Multi-Sport Scraper...")
        
        try:
            start_date, end_date = date_range
            matches = []
            
            # Fetch for each day in range
            current_date = start_date
            while current_date <= end_date:
                scraped_matches = self.multi_sport_scraper.scrape_daily_matches(current_date, sports)
                
                for scraped_match in scraped_matches:
                    match = UnifiedMatchData(
                        match_id=scraped_match.match_id,
                        sport=scraped_match.sport,
                        league=scraped_match.league,
                        home_team=scraped_match.home_team,
                        away_team=scraped_match.away_team,
                        match_time=scraped_match.match_time,
                        venue=scraped_match.venue,
                        team_stats=scraped_match.home_team_stats,
                        historical_data={
                            'head_to_head': scraped_match.head_to_head,
                            'recent_form': scraped_match.recent_form
                        },
                        external_factors={
                            'weather': scraped_match.weather,
                            'injuries': scraped_match.injuries,
                            'suspensions': scraped_match.suspensions
                        },
                        sources=['multi_sport_scraper'],
                        data_quality=scraped_match.data_quality
                    )
                    matches.append(match)
                
                current_date += timedelta(days=1)
            
            logger.info(f"‚úÖ Multi-Sport Scraper: {len(matches)} matches")
            return matches
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching from Multi-Sport Scraper: {e}")
            return []
    
    def _normalize_sport(self, sport_key: str) -> str:
        """Normalize sport key to standard sport name"""
        mapping = {
            'soccer_epl': 'football',
            'soccer_spain_la_liga': 'football',
            'soccer_germany_bundesliga': 'football',
            'soccer_italy_serie_a': 'football',
            'soccer_france_ligue_one': 'football',
            'soccer_uefa_champs_league': 'football',
            'tennis_atp': 'tennis',
            'tennis_wta': 'tennis',
            'basketball_nba': 'basketball',
            'icehockey_nhl': 'ice_hockey'
        }
        return mapping.get(sport_key, sport_key)
    
    def _merge_duplicate_matches(self, matches: List[UnifiedMatchData]) -> List[UnifiedMatchData]:
        """Merge duplicate matches from different sources"""
        match_dict = {}
        
        for match in matches:
            # Create unique key based on teams and time
            key = f"{match.sport}_{match.home_team}_{match.away_team}_{match.match_time.date()}"
            
            if key not in match_dict:
                match_dict[key] = match
            else:
                # Merge data from different sources
                existing_match = match_dict[key]
                
                # Merge odds data
                if match.odds_data and not existing_match.odds_data:
                    existing_match.odds_data = match.odds_data
                
                # Merge team stats
                if match.team_stats and not existing_match.team_stats:
                    existing_match.team_stats = match.team_stats
                
                # Merge historical data
                if match.historical_data and not existing_match.historical_data:
                    existing_match.historical_data = match.historical_data
                
                # Merge external factors
                if match.external_factors and not existing_match.external_factors:
                    existing_match.external_factors = match.external_factors
                
                # Update sources
                existing_match.sources.extend(match.sources)
                existing_match.sources = list(set(existing_match.sources))
                
                # Update data quality (average)
                existing_match.data_quality = (existing_match.data_quality + match.data_quality) / 2
        
        return list(match_dict.values())
    
    async def fetch_match_statistics(self, match: UnifiedMatchData) -> UnifiedMatchData:
        """
        Hae lis√§tilastot ottelulle
        
        Args:
            match: UnifiedMatchData objekti
            
        Returns:
            P√§ivitetty UnifiedMatchData objekti tilastoilla
        """
        logger.info(f"üìä Fetching statistics for {match.home_team} vs {match.away_team}")
        
        # This would fetch detailed statistics
        # Implementation depends on available APIs and scrapers
        
        return match
    
    async def fetch_historical_data(self, match: UnifiedMatchData) -> UnifiedMatchData:
        """
        Hae historiallinen data ottelulle (H2H, muoto, jne.)
        
        Args:
            match: UnifiedMatchData objekti
            
        Returns:
            P√§ivitetty UnifiedMatchData objekti historiallisella datalla
        """
        logger.info(f"üìú Fetching historical data for {match.home_team} vs {match.away_team}")
        
        # This would fetch historical data
        # Implementation depends on available APIs and scrapers
        
        return match


async def main():
    """Test Unified Data Fetcher"""
    print("üìä UNIFIED DATA FETCHER TEST")
    print("=" * 50)
    
    fetcher = UnifiedDataFetcher()
    
    # Fetch matches
    print("\nüîç Fetching matches...")
    matches = await fetcher.fetch_all_matches(['tennis', 'football'])
    
    print(f"‚úÖ Fetched {len(matches)} matches")
    
    if matches:
        print("\nüìä Sample matches:")
        for match in matches[:3]:
            print(f"\n‚Ä¢ {match.home_team} vs {match.away_team}")
            print(f"  Sport: {match.sport}")
            print(f"  League: {match.league}")
            print(f"  Time: {match.match_time}")
            print(f"  Sources: {', '.join(match.sources)}")
            print(f"  Data Quality: {match.data_quality:.2f}")


if __name__ == "__main__":
    asyncio.run(main())

